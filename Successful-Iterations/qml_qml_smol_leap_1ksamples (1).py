# -*- coding: utf-8 -*-
"""QML_QML_SMoL_Leap-1kSamples.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17JCyK8uX1lmgmors9fPhoiKpdfmbEk_M
"""

# Quantum-Classical Hybrid Model using BERT and Quantum Kernel SVM for GoEmotions Dataset

# Step 1: Environment
!pip uninstall -y jax jaxlib
!pip install jax==0.4.28 jaxlib==0.4.28 --quiet
!pip install pennylane seaborn tensorflow-datasets scikit-learn==1.6.1 transformers --upgrade --quiet

# Step 2: Libraries
import pennylane as qml
from pennylane import numpy as np
import numpy as onp
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from transformers import BertTokenizer, BertModel
import tensorflow_datasets as tfds
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# Step 3: Dataset
print("Loading GoEmotions dataset...")
dataset, info = tfds.load('goemotions', with_info=True)
train_dataset = dataset['train']

# Define emotion labels in the correct order
emotion_labels = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',
    'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',
    'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral',
    'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise'
]

texts, labels = [], []
for example in tfds.as_numpy(train_dataset):
    texts.append(example['comment_text'].decode('utf-8'))
    for idx, label in enumerate(emotion_labels):
        if example[label]:
            labels.append(idx)
            break
    else:
        labels.append(20)

n_classes = 28

# Step 4: BERT
print("Extracting BERT embeddings...")
from tqdm import tqdm

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Function to compute BERT [CLS] embeddings
def bert_embed(sentences):
    with torch.no_grad():
        inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)
        outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :].numpy()

batch_size = 64
bert_embeddings = []

# tqdm to track progress
for i in tqdm(range(0, len(texts), batch_size), desc="Extracting BERT batches"):
    bert_embeddings.append(bert_embed(texts[i:i+batch_size]))

X = onp.vstack(bert_embeddings)
y = onp.array(labels)

# Saved Step 4 recovery

from google.colab import drive
drive.mount('/content/drive')


import numpy as onp

# Load BERT embeddings and labels
save_dir = '/content/drive/MyDrive/q_kernel_project/'

X_train = onp.load(save_dir + 'X_train.npy')
X_test = onp.load(save_dir + 'X_test.npy')
y_train = onp.load(save_dir + 'y_train.npy')
y_test = onp.load(save_dir + 'y_test.npy')

print("BERT embeddings and labels loaded successfully.")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

# Now Step 5: If PCA hasn't been done earlier then SKIP
# Can directly proceed to Step 6

# Step 5: PCA
print("Reducing BERT embedding dimensions using PCA...")
pca = PCA(n_components=16)
X_reduced = pca.fit_transform(X)

# Limit dataset to 5000 samples for computational feasibility
X = X[:1000]
y = y[:1000]

pca = PCA(n_components=16)
X_reduced = pca.fit_transform(X)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)

# SKIP Step 5 from Recovery

import numpy as onp

save_dir = '/content/drive/MyDrive/q_kernel_project/'

#  Load saved PCA-transformed BERT train/test splits
X_train = onp.load(save_dir + 'X_train.npy')
X_test = onp.load(save_dir + 'X_test.npy')
y_train = onp.load(save_dir + 'y_train.npy')
y_test = onp.load(save_dir + 'y_test.npy')

print("Loaded preprocessed BERT data from Drive:")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

# Step 6: Quantum Kernel Setup
n_qubits = 16
dev = qml.device('default.qubit', wires=n_qubits)

def feature_map(x):
    for i in range(n_qubits):
        qml.RY(x[i], wires=i)
    for i in range(n_qubits - 1):
        qml.CNOT(wires=[i, i + 1])
    qml.CNOT(wires=[n_qubits - 1, 0])

@qml.qnode(dev)
def kernel_circuit(x1, x2):
    feature_map(x1)
    qml.adjoint(feature_map)(x2)
    return qml.probs(wires=range(n_qubits))

# Quantum kernel function
from tqdm import tqdm

import pickle

def quantum_kernel(X1, X2):
    kernel = np.zeros((len(X1), len(X2)))
    for i in tqdm(range(len(X1)), desc='Computing quantum kernel rows'):
        if i % (len(X1) // 15) == 0 and i != 0:
            with open(f'kernel_checkpoint_row_{i}.pkl', 'wb') as f:
                pickle.dump(kernel, f)
            print(f'Checkpoint saved at row {i}')
        for j in range(len(X2)):
            kernel[i, j] = np.abs(kernel_circuit(X1[i], X2[j])[0])
    return kernel

# STEP 7: Resume-safe Quantum Kernel Computation + Save All Checkpoints and Embeddings to Google Drive : GPT used for code functionality confirmation and the saving feature

import os, pickle, glob, re
from tqdm import tqdm

#  1. Mount Google Drive (run this once per session)
from google.colab import drive
drive.mount('/content/drive')

#  2. Define paths
save_dir = '/content/drive/MyDrive/q_kernel_project/'
checkpoint_path = os.path.join(save_dir, 'checkpoints/')
os.makedirs(checkpoint_path, exist_ok=True)

#  3. Save BERT outputs to Drive for future reuse (only once if not already saved)
onp.save(os.path.join(save_dir, 'X_train.npy'), X_train)
onp.save(os.path.join(save_dir, 'X_test.npy'), X_test)
onp.save(os.path.join(save_dir, 'y_train.npy'), y_train)
onp.save(os.path.join(save_dir, 'y_test.npy'), y_test)

print("BERT embeddings saved to Drive.")

#  4. Kernel computation function with Drive-safe resume
def quantum_kernel(X1, X2, checkpoint_prefix='kernel_checkpoint_row_', resume=True):
    kernel = np.zeros((len(X1), len(X2)))
    start_i = 0

    if resume:
        checkpoint_files = glob.glob(os.path.join(checkpoint_path, f'{checkpoint_prefix}*.pkl'))
        if checkpoint_files:
            last_row = max([int(re.findall(r'\d+', f)[-1]) for f in checkpoint_files])
            with open(os.path.join(checkpoint_path, f'{checkpoint_prefix}{last_row}.pkl'), 'rb') as f:
                kernel = pickle.load(f)
            start_i = last_row +1
            print(f"Resuming from row {start_i}")
        else:
            print("No previous checkpoint found. Starting from scratch.")

    for i in tqdm(range(start_i, len(X1)), desc='Computing quantum kernel rows'):
        if i % (len(X1) // 50) == 0 and i != 0:
            with open(os.path.join(checkpoint_path, f'{checkpoint_prefix}{i}.pkl'), 'wb') as f:
                pickle.dump(kernel, f)
            print(f' Checkpoint saved at row {i}')
        for j in range(len(X2)):
            kernel[i, j] = np.abs(kernel_circuit(X1[i], X2[j])[0])

     #  Final save after all rows are done
    final_row = len(X1)
    with open(os.path.join(checkpoint_path, f'{checkpoint_prefix}{final_row}.pkl'), 'wb') as f:
        pickle.dump(kernel, f)
    print(f"Final checkpoint saved at row {final_row}")
    return kernel

#  5. Run K_train (you can pause/restart/resume from Drive anytime)
print(" Starting/resuming K_train computation...")
K_train = quantum_kernel(X_train, X_train)

#  6. Run K_test after K_train
print(" Starting/resuming K_test computation...")
K_test = quantum_kernel(X_test, X_train, checkpoint_prefix='K_test_row_')

#  7. Save final kernel matrices to Drive
with open(os.path.join(save_dir, 'K_train_final.pkl'), 'wb') as f:
    pickle.dump(K_train, f)

with open(os.path.join(save_dir, 'K_test_final.pkl'), 'wb') as f:
    pickle.dump(K_test, f)

print(" Kernel matrices saved to Drive.")

# Step 8: Train SVM with Precomputed Quantum Kernel
print("Training Quantum Kernel SVM...")
svm = SVC(kernel='precomputed')
svm.fit(K_train, y_train)

# Step 9: Evaluation
y_pred = svm.predict(K_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.2f}")

# Step 10: Confusion Matrix
plt.figure(figsize=(12,10))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Various other plots

from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve
)
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

#  1. Predict
y_pred = svm.predict(K_test)

#  2. Print Classification Report
print("\n Classification Report:")
unique_labels = sorted(np.unique(np.concatenate([y_test, y_pred])))
used_labels = [emotion_labels[i] for i in unique_labels]
print(classification_report(y_test, y_pred, labels=unique_labels, target_names=used_labels))

#  3. Accuracy, Precision, Recall, F1
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

print(f"\n Accuracy: {accuracy:.4f}")
print(f" Precision (weighted): {precision:.4f}")
print(f" Recall (weighted): {recall:.4f}")
print(f" F1 Score (weighted): {f1:.4f}")

#  4. Confusion Matrix Heatmap
plt.figure(figsize=(14, 12))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=used_labels, yticklabels=used_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title(" Confusion Matrix")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

#  5. Precision-Recall Curve for each class (optional, slow for many classes)
from sklearn.preprocessing import label_binarize

if len(np.unique(y_test)) <= 10:
    y_test_bin = label_binarize(y_test, classes=np.arange(len(emotion_labels)))
    y_pred_bin = label_binarize(y_pred, classes=np.arange(len(emotion_labels)))

    plt.figure(figsize=(10, 8))
    for i in range(y_test_bin.shape[1]):
        precision_i, recall_i, _ = precision_recall_curve(y_test_bin[:, i], y_pred_bin[:, i])
        plt.plot(recall_i, precision_i, label=f"{emotion_labels[i]}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve (per class)")
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("Skipped per-class precision-recall curve (too many classes)")